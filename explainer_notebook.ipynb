{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import ast\n",
    "import netwulf as nw\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'Insert GitHub Token Here'\n",
    "\n",
    "headers = {\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "    'Authorization': f'token {token}',\n",
    "}\n",
    "repositories = []\n",
    "\n",
    "def page_request(page, params, headers):\n",
    "    base_url = 'https://api.github.com/search/repositories'\n",
    "    contributirs = []\n",
    "    params['page'] = page\n",
    "\n",
    "    # Send a GET request to the GitHub API\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    response.raise_for_status()  # Check for errors\n",
    "\n",
    "    # Add the repositories from this page to the list\n",
    "    repositories.extend(response.json()['items'])\n",
    "\n",
    "    \n",
    "    return repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'q': 'stars:>1 topic:machine-learning',  # Search for repositories with more than 1 star and the topic \"machine-learning\"\n",
    "    'sort': 'stars',  # Sort by the number of stars\n",
    "    'order': 'desc',  # Order in descending order (most stars first)\n",
    "    'per_page': 100,  # Get 100 results per page\n",
    "}\n",
    "\n",
    "output = Parallel(n_jobs=4)(delayed(page_request)(page, params, headers) for page in tqdm(range(1, 11)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'q': 'stars:>1 topic:machine-learning',  # Search for repositories with more than 1 star and the topic \"machine-learning\"\n",
    "    'sort': 'stars',  # Sort by the number of stars\n",
    "    'order': 'desc',  # Order in descending order (most stars first)\n",
    "    'per_page': 100,  # Get 100 results per page\n",
    "}\n",
    "\n",
    "output = Parallel(n_jobs=4)(delayed(page_request)(page, params, headers) for page in tqdm(range(1, 11)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate unique repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repositories = [item for sublist in output for item in sublist]\n",
    "len(repositories)\n",
    "\n",
    "repositories2 = [item for sublist in output2 for item in sublist]\n",
    "len(repositories2)\n",
    "\n",
    "combine = repositories + repositories2\n",
    "print(len(combine))\n",
    "\n",
    "description = []\n",
    "# check the unique number of repositories\n",
    "unique_repositories = []\n",
    "seen_ids = set()\n",
    "\n",
    "for repo in combine:\n",
    "    if repo['id'] not in seen_ids:\n",
    "        unique_repositories.append(repo)\n",
    "        seen_ids.add(repo['id'])\n",
    "\n",
    "print(len(unique_repositories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make df with repository name and full name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "full_name = []\n",
    "for repo in unique_repositories:\n",
    "    name.append(repo['name'])\n",
    "    full_name.append(repo['full_name'])\n",
    "\n",
    "df_name = pd.DataFrame()\n",
    "df_name['name'] = name[:len(name)-1]\n",
    "df_name['full_name'] = full_name[:len(name)-1]\n",
    "\n",
    "# save the dataframe\n",
    "df_name.to_csv('repositories_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributors(repo):  \n",
    "    headers = {\n",
    "    'Accept': 'application/vnd.github.v3+json',\n",
    "    'Authorization': f'token {token}',\n",
    "    }\n",
    "    \n",
    "    topics = []\n",
    "    description = []\n",
    "    stargazers_count = []\n",
    "    company = []\n",
    "    c = []\n",
    "    \n",
    "    # check if in unique repositories\n",
    "    \n",
    "    c.append(repo['owner']['login'])\n",
    "    \n",
    "    topics.append(repo['topics'])\n",
    "    description.append(repo['description'])\n",
    "    stargazers_count.append(repo['stargazers_count']) \n",
    "\n",
    "    url = f\"https://api.github.com/repos/{repo['owner']['login']}/{repo['name']}/contributors\"\n",
    "    \n",
    "    all_contributors = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url + '?page=' + str(page), headers=headers)\n",
    "        \n",
    "        # If the request was successful\n",
    "        if response.status_code == 200:\n",
    "            contri = response.json()\n",
    "            \n",
    "            if not contri:\n",
    "                break\n",
    "            i = 0\n",
    "            while i < len(contri):\n",
    "                \n",
    "                contributor = contri[i]\n",
    "                c.append(contributor['login'])\n",
    "                user_response = requests.get(contributor['url'], headers=headers)\n",
    "                if user_response.status_code == 200:\n",
    "                    user = user_response.json()\n",
    "                    if 'company' in user:\n",
    "                        company.append(user['company'])\n",
    "                    else:\n",
    "                        company.append('None')\n",
    "                    i += 1  # Increment the index\n",
    "\n",
    "                elif user_response.status_code == 403:  # Rate limit exceeded\n",
    "                    reset_time = int(user_response.headers['X-RateLimit-Reset'])  # Get the time when the rate limit will reset\n",
    "                    sleep_time = max(reset_time - time.time(), 0)  # Calculate how long to sleep\n",
    "                    print(f'Rate limit exceeded. Sleeping for {sleep_time} seconds.')\n",
    "                    time.sleep(sleep_time)  # Sleep until the rate limit resets\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "            page += 1\n",
    "        elif response.status_code == 403:  # Rate limit exceeded\n",
    "            reset_time = int(response.headers['X-RateLimit-Reset'])  # Get the time when the rate limit will reset\n",
    "            sleep_time = max(reset_time - time.time(), 0)  # Calculate how long to sleep\n",
    "            print(f'Rate limit exceeded. Sleeping for {sleep_time} seconds.')\n",
    "            time.sleep(sleep_time)  # Sleep until the rate limit resets\n",
    "            continue  # Try the request again\n",
    "        else:\n",
    "            break\n",
    "    all_contributors.append(c)\n",
    "\n",
    "\n",
    "    return all_contributors, topics, description, stargazers_count, company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Parallel(n_jobs=4)(delayed(get_contributors)(repo) for repo in tqdm(unique_repositories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contri = []\n",
    "topics = []\n",
    "description = []\n",
    "stargazers_count = []\n",
    "company = []\n",
    "\n",
    "for i in range(len(output)):\n",
    "    contri.append(output[i][0][0])\n",
    "    topics.append(output[i][1][0])\n",
    "    description.append(output[i][2][0])\n",
    "    stargazers_count.append(output[i][3][0])\n",
    "    company.append(output[i][4][0])\n",
    "\n",
    "print(len(contri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the owners of the repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "contributor = []\n",
    "url = \"https://api.github.com/users/\"\n",
    "\n",
    "for i in tqdm(range(len(output))):\n",
    "    for j in range(len(output[i][4])):\n",
    "        if j == 0:\n",
    "            while True:\n",
    "                # add code to make api call about output[i][0][0][j]\n",
    "                url =\"https://api.github.com/users/\"+ output[i][0][0][j]\n",
    "                user_response = requests.get(url, headers=headers)\n",
    "                if user_response.status_code==200:\n",
    "                    company.append(user_response.json()['company'])\n",
    "                    break\n",
    "                elif user_response.status_code == 403:  # Rate limit exceeded\n",
    "                    reset_time = int(user_response.headers['X-RateLimit-Reset'])  # Get the time when the rate limit will reset\n",
    "                    sleep_time = max(reset_time - time.time(), 0)  # Calculate how long to sleep\n",
    "                    time.sleep(sleep_time)  # Sleep until the rate limit resets\n",
    "                \n",
    "\n",
    "        elif output[i][4][j] == None:\n",
    "            company.append(\"No Org\")\n",
    "        else:\n",
    "            company.append(output[i][4][j])\n",
    "        \n",
    "        contributor.append(output[i][0][0][j])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if company and contributor is same len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(company))\n",
    "print(len(contributor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['contributors'] = contri\n",
    "df['topics'] = topics\n",
    "df['description'] = description\n",
    "df['stargazers_count'] = stargazers_count\n",
    "\n",
    "df.to_csv('repositories.csv', index=False)\n",
    "\n",
    "contri_df = pd.DataFrame({\n",
    "    'Contributor': contributor,\n",
    "    'Company': company\n",
    "})\n",
    "\n",
    "contri_df['Company'][contri_df['Company'].isna()] = \"No Org\"\n",
    "\n",
    "\n",
    "contri_df.to_csv('contributors.csv', index=False)\n",
    "\n",
    "df.head()  # Display the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributors = pd.read_csv('contributors.csv')\n",
    "df_repositories = pd.read_csv('repositories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure df list elements are list and that there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contributors = df_contributors.drop_duplicates('Contributor')\n",
    "\n",
    "df_repositories[\"contributors\"] = df_repositories[\"contributors\"].apply(ast.literal_eval)\n",
    "df_repositories['topics'] = df_repositories['topics'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If company is \"No Org\", then inset identity key\n",
    "identity_key_generator = iter(range(1, len(df_contributors) + 1))\n",
    "df_contributors['Company'] = df_contributors['Company'].apply(lambda x: str(next(identity_key_generator)) if x == \"No Org\" else x)\n",
    "\n",
    "# Display the first few rows\n",
    "display(df_contributors)\n",
    "\n",
    "# Expand the contributors list\n",
    "df_repositories_expanded = df_repositories.explode('contributors')\n",
    "\n",
    "# Display the first few rows\n",
    "display(df_repositories_expanded.head())\n",
    "\n",
    "# Merge the repositories and contributors dataframes\n",
    "df_merged = pd.merge(df_repositories_expanded, df_contributors, left_on='contributors', right_on='Contributor', how='left')\n",
    "\n",
    "# Drop the 'Contributor' column and rename the 'contributors' column and 'Company' column\n",
    "df_merged = df_merged.drop(columns=['Contributor'])\n",
    "df_merged = df_merged.rename(columns={'contributors': 'contributor'})\n",
    "df_merged = df_merged.rename(columns={'Company': 'company'})\n",
    "\n",
    "# Display the first few rows\n",
    "display(df_merged.head())\n",
    "\n",
    "# Print the shape of all dataframes\n",
    "print(f\"Contributors: {df_contributors.shape}\")\n",
    "print(f\"Repositories: {df_repositories.shape}\")\n",
    "print(f\"Repositories Expanded: {df_repositories_expanded.shape}\")\n",
    "print(f\"Merged: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process company collumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slight processing of company to try and fix most comman issues\n",
    "\n",
    "df_merged['company'] = df_merged['company'].astype(str)\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace('Inc','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace('https://github.com/','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace('\\'','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].apply(lambda x: 'google' if 'google' in x.lower() else x.lower())\n",
    "\n",
    "df_merged['company'] = df_merged['company'].apply(lambda x: 'facebook' if 'facebook' in x.lower() else x.lower())\n",
    "\n",
    "df_merged['company'] = df_merged['company'].apply(lambda x: x.encode(\"ascii\", \"ignore\").decode())\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace(' ','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].str.replace('@','')\n",
    "\n",
    "df_merged['company'] = df_merged['company'].apply(lambda x: 'tensorflow' if 'tensorflow' in x.lower() else x.lower())\n",
    "\n",
    "df_dict = df_merged.set_index('contributor')['company'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create adj list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list2 = defaultdict(int)\n",
    "count_of_total_contributors = 0\n",
    "for row in df_repositories.iterrows():\n",
    "    # check if in unique repositories\n",
    "    # For each pair of contributors\n",
    "    for i in range(len(row[1]['contributors'])):\n",
    "        for j in range(i + 1, len(row[1]['contributors'])):\n",
    "            # Get the logins of the contributors\n",
    "            login1 = row[1]['contributors'][i]\n",
    "            login2 = row[1]['contributors'][j]\n",
    "            adj_list2[(login1, login2)] += 1\n",
    "            count_of_total_contributors += 1\n",
    "        \n",
    "# print max value of the pair\n",
    "max_value = max(adj_list2.values())\n",
    "print(f\"Max value: {max_value}\")\n",
    "print(min(adj_list2.values()))\n",
    "# avarage value\n",
    "print(f\"avarage value: {sum(adj_list2.values())/len(adj_list2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph with attribute and get number of unique companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "weighted_edge_list = [(u, v, d) for (u, v), d in adj_list2.items() if d > 1]\n",
    "\n",
    "G.add_weighted_edges_from(weighted_edge_list)\n",
    "\n",
    "nx.set_node_attributes(G, df_dict, 'company')\n",
    "\n",
    "# get uniqie companies in the graph using G\n",
    "companies = set(nx.get_node_attributes(G, 'company').values())\n",
    "print(f\"Number of companies: {len(companies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic network analysis - with bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of nodes (authors)\n",
    "num_nodes = G.number_of_nodes()\n",
    "\n",
    "# Total number of links (collaborations)\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "# Network's density\n",
    "density = nx.density(G)\n",
    "\n",
    "# Check if the network is fully connected\n",
    "is_connected = nx.is_connected(G)\n",
    "\n",
    "# Number of connected components\n",
    "num_connected_components = nx.number_connected_components(G)\n",
    "\n",
    "# Number of isolated nodes\n",
    "num_isolated_nodes = nx.number_of_isolates(G)\n",
    "\n",
    "# largest connected component\n",
    "largest_cc = max(nx.connected_components(G), key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Density: {density}\")\n",
    "print(f\"Is the network fully connected? {'Yes' if is_connected else 'No'}\")\n",
    "print(f\"Number of connected components: {num_connected_components}\")\n",
    "print(f\"Number of isolated nodes: {num_isolated_nodes}\")\n",
    "print(f\"largest connected component: {len(largest_cc)}\")\n",
    "\n",
    "weights = [data['weight'] for u, v, data in G.edges(data=True)]\n",
    "\n",
    "# Calculate the maximum, minimum, and average weight\n",
    "max_weight = max(weights)\n",
    "min_weight = min(weights)\n",
    "avg_weight = sum(weights) / len(weights)\n",
    "\n",
    "print(f\"Maximum weight: {max_weight}\")\n",
    "print(f\"Minimum weight: {min_weight}\")\n",
    "print(f\"Average weight: {avg_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avarage degree\n",
    "degree = dict(G.degree())\n",
    "avg_degree = sum(degree.values()) / len(degree)\n",
    "print(f\"Average degree: {avg_degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The top 10 highest degree nodes are: {sorted(degree.items(), key=lambda x: x[1], reverse=True)[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis containing random networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the propability p for the Computational Social Scientists network\n",
    "num_nodes = len(G.nodes)\n",
    "num_edges = len(G.edges)\n",
    "p = num_edges / (num_nodes * (num_nodes - 1) / 2)\n",
    "print(f'number of nodes: {num_nodes} and number of edges: {num_edges}')\n",
    "print(f'The edge probability for the Computational Social Scientists network is {p}')\n",
    "\n",
    "Gr = nx.erdos_renyi_graph(num_nodes, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = dict(G.degree())\n",
    "degree_randomr = dict(Gr.degree())\n",
    "\n",
    "avarage_degree = np.mean(list(degree.values())) \n",
    "avarage_degree_randomr = np.mean(list(degree_randomr.values()))\n",
    "\n",
    "print(f'The average degree of github collabaration network {avarage_degree}')\n",
    "print(f'The average degree of the Random Network is {avarage_degree_randomr}')\n",
    "\n",
    "max_degree = max(degree, key=degree.get)\n",
    "max_degree_randomr = max(degree_randomr, key=degree_randomr.get)\n",
    "\n",
    "print(f'The node with the highest degree in the github collabaration network is {max_degree} with a degree of {degree[max_degree]}')\n",
    "print(f'The node with the highest degree in the Random Network is {max_degree_randomr} with a degree of {degree_randomr[max_degree_randomr]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the degree distribution for the Computational Social Scientists network\n",
    "degrees_real = [degree for node, degree in G.degree()]\n",
    "\n",
    "print(f'number of nodes with degree 1: {len([degree for degree in degrees_real if degree == 1])}')\n",
    "print(f'number of nodes with degree 2: {len([degree for degree in degrees_real if degree == 2])}')\n",
    "\n",
    "# Compute the degree distribution for the random network\n",
    "degrees_random = [degree for node, degree in Gr.degree()]\n",
    "\n",
    "# Plot the degree distributions\n",
    "plt.figure()\n",
    "plt.hist(degrees_random, bins=50, density=True ,alpha=0.7, label='Random Network')\n",
    "plt.hist(degrees_real, bins=3*10**3, density=True, alpha=0.7, label='Real Network')\n",
    "\n",
    "# Add vertical lines for the average degrees\n",
    "plt.axvline(np.mean(degrees_random), color='r', linestyle='--', label='Average degree (random network)')\n",
    "plt.axvline(np.mean(degrees_real), color='b', linestyle='--', label='Average degree (real network)')\n",
    "\n",
    "# log scale the x-axis\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.title('Degree Distributions')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assorsativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree and attribute assortativity\n",
    "degree_correlation = nx.degree_assortativity_coefficient(G)\n",
    "attribute_correlation = nx.attribute_assortativity_coefficient(G, 'company')\n",
    "\n",
    "print(f'Degree Assortativity: {degree_correlation}')\n",
    "print(f'Attribute Assortativity: {attribute_correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configuration_model(G: nx.Graph, num_swaps: int):\n",
    "    \"\"\"\n",
    "    Function to generate a configuration model of a given network.\n",
    "\n",
    "    Args:\n",
    "        G: nx.Graph, Network to consider\n",
    "        num_swaps: int, Number of edge swaps to perform\n",
    "    \n",
    "    Returns:\n",
    "        G_new: nx.Graph, Configuration model of the network\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an exact copy of the original network\n",
    "    G_new = G.copy()\n",
    "\n",
    "    # Perform edge swaps\n",
    "    G_new = nx.double_edge_swap(G_new, nswap=num_swaps, max_tries=num_swaps*2)\n",
    "\n",
    "    degree_assortativity = nx.degree_assortativity_coefficient(G_new)\n",
    "    \n",
    "    assorsativity = nx.attribute_assortativity_coefficient(G_new, 'company')\n",
    "\n",
    "    return G_new, degree_assortativity, assorsativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'The degree assortativity coefficient for the github collaboration network is {nx.degree_assortativity_coefficient(G)}')\n",
    "\n",
    "num_edges = G.number_of_edges()\n",
    "num_networks = 100\n",
    "\n",
    "# use tqdm and parallel to run congifuration model\n",
    "output = Parallel(n_jobs=4)(delayed(configuration_model)(G, num_edges * 10) for _ in tqdm(range(num_networks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_assortativity_config = []\n",
    "attribute_assortativity_config = []\n",
    "for i in range(len(output)):\n",
    "    degree_assortativity_config.append(output[i][1])\n",
    "    attribute_assortativity_config.append(output[i][2])\n",
    "    \n",
    "\n",
    "print(f'lenght of degree_assortativity_config: {len(degree_assortativity_config)}')\n",
    "print(f'lenght of attribute_assortativity_config: {len(attribute_assortativity_config)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot Degree Assortativity\n",
    "axs[0].hist(degree_assortativity_config, bins=20, alpha=0.5, label='Random model')\n",
    "axs[0].axvline(degree_correlation, color='r', linestyle='dashed', linewidth=2, label='Original network')\n",
    "axs[0].set_xlabel('Assortativity coefficient')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Degree Assortativity')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Attribute Assortativity\n",
    "axs[1].hist(attribute_assortativity_config, bins=20, alpha=0.5, label='Random model')\n",
    "axs[1].axvline(attribute_correlation, color='r', linestyle='dashed', linewidth=2, label='Original network')\n",
    "axs[1].set_xlabel('Assortativity coefficient')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Attribute Assortativity')\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### illustrate network and random network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network = nw.get_filtered_network(G,node_group_key='company')\n",
    "stylized_network, config = nw.visualize(stylized_network)\n",
    "# illustrative example of the network stylized_network\n",
    "nw.draw_netwulf(stylized_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network = nw.get_filtered_network(G)\n",
    "stylized_network, config = nw.visualize(stylized_network)\n",
    "# illustrative example of the network stylized_network\n",
    "nw.draw_netwulf(stylized_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network = nw.get_filtered_network(Gr)\n",
    "stylized_network, config = nw.visualize(stylized_network)\n",
    "# illustrative example of the network stylized_network\n",
    "nw.draw_netwulf(stylized_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest component and shortest parth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest component of the network\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "largest_cc_random = max(nx.connected_components(Gr), key=len)\n",
    "\n",
    "# Create subgraphs\n",
    "largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "largest_cc_random_subgraph = Gr.subgraph(largest_cc_random)\n",
    "\n",
    "print(f'The largest connected component of the github collaboration network has {len(largest_cc)} nodes')\n",
    "print(f'The largest connected component of the Random Network has {len(largest_cc_random)} nodes')\n",
    "\n",
    "# see avarage shortest path\n",
    "shortest_path = nx.average_shortest_path_length(largest_cc_subgraph)\n",
    "shortest_path_random = nx.average_shortest_path_length(largest_cc_random_subgraph)\n",
    "\n",
    "print(f'The average shortest path length of the github collaboration network is {shortest_path}')\n",
    "print(f'The average shortest path length of the Random Network is {shortest_path_random}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove bot node and edge from the network (node with most degree)\n",
    "GB = G.copy()\n",
    "GB.remove_node(max_degree)\n",
    "\n",
    "# Do the same analysis\n",
    "num_nodes = GB.number_of_nodes()\n",
    "num_edges = GB.number_of_edges()\n",
    "density = nx.density(GB)\n",
    "is_connected = nx.is_connected(GB)\n",
    "num_connected_components = nx.number_connected_components(GB)\n",
    "num_isolated_nodes = nx.number_of_isolates(GB)\n",
    "largest_cc = max(nx.connected_components(GB), key=len)\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Density: {density}\")\n",
    "print(f\"Is the network fully connected? {'Yes' if is_connected else 'No'}\")\n",
    "print(f\"Number of connected components: {num_connected_components}\")\n",
    "print(f\"Number of isolated nodes: {num_isolated_nodes}\")\n",
    "print(f\"largest connected component: {len(largest_cc)}\")\n",
    "\n",
    "# Compute the propability p for the Computational Social Scientists network\n",
    "num_nodes = len(GB.nodes)\n",
    "num_edges = len(GB.edges)\n",
    "p = num_edges / (num_nodes * (num_nodes - 1) / 2)\n",
    "print(f'number of nodes: {num_nodes} and number of edges: {num_edges}')\n",
    "print(f'The edge probability for the Computational Social Scientists network is {p}')\n",
    "\n",
    "Grb = nx.erdos_renyi_graph(num_nodes, p)\n",
    "\n",
    "degree = dict(GB.degree())\n",
    "degree_randomrm = dict(Grb.degree())\n",
    "\n",
    "degree_list_real = list(degree.values())\n",
    "degree_list_random = list(degree_randomrm.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avarage_degree = np.mean(list(degree.values())) \n",
    "avarage_degree_randomrm = np.mean(list(degree_randomrm.values()))\n",
    "\n",
    "print(f'The average degree of github collabaration network {avarage_degree}')\n",
    "print(f'The average degree of the Random Network is {avarage_degree_randomrm}')\n",
    "\n",
    "max_degree = max(degree, key=degree.get)\n",
    "max_degree_randomrm = max(degree_randomrm, key=degree_randomrm.get)\n",
    "\n",
    "print(f'The node with the highest degree in the github collabaration network is {max_degree} with a degree of {degree[max_degree]}')\n",
    "print(f'The node with the highest degree in the Random Network is {max_degree_randomrm} with a degree of {degree_randomrm[max_degree_randomrm]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(degree_list_random, bins=50, density=True ,alpha=0.7, label='Random Network')\n",
    "plt.hist(degree_list_real, bins=5*10**2, density=True, alpha=0.7, label='Real Network')\n",
    "\n",
    "# Add vertical lines for the average degrees\n",
    "plt.axvline(np.mean(degree_list_random), color='r', linestyle='--', label='Average degree (random network)')\n",
    "plt.axvline(np.mean(degree_list_real), color='b', linestyle='--', label='Average degree (real network)')\n",
    "\n",
    "# log scale the x-axis\n",
    "plt.xscale('log')\n",
    "\n",
    "# Set xticks to only integers\n",
    "plt.gca().xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\n",
    "\n",
    "# Set x-axis limits\n",
    "plt.xlim([0.9, max(max(degree_list_random), max(degree_list_real))])\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Degree Distributions - Graph without Bot Node')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('degree_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient = nx.average_clustering(G)\n",
    "clustering_coefficient_random = nx.average_clustering(Gr)\n",
    "\n",
    "print(f'The clustering coefficient of the github collaboration network is {clustering_coefficient}')\n",
    "print(f'The clustering coefficient of the Random Network is {clustering_coefficient_random}')\n",
    "random_clustering = []\n",
    "for i in tqdm(range(100)):\n",
    "    # generate a random network\n",
    "    GrCluster = nx.erdos_renyi_graph(num_nodes, p)\n",
    "    random_clustering.append(nx.average_clustering(GrCluster))\n",
    "\n",
    "print(f'The average clustering coefficient of the Random Network is {np.mean(random_clustering)}')\n",
    "print(f'The standard deviation of the clustering coefficient of the Random Network is {np.std(random_clustering)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_correlation = nx.degree_assortativity_coefficient(GB)\n",
    "attribute_correlation = nx.attribute_assortativity_coefficient(GB, 'company')\n",
    "\n",
    "print(f'Degree Assortativity: {degree_correlation}')\n",
    "print(f'Attribute Assortativity: {attribute_correlation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges = G.number_of_edges()\n",
    "num_networks = 100\n",
    "\n",
    "# use tqdm and parallel to run congifuration model\n",
    "output = Parallel(n_jobs=4)(delayed(configuration_model)(GB, num_edges * 10) for _ in tqdm(range(num_networks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_assortativity_config_B = []\n",
    "attribute_assortativity_config_B = []\n",
    "for i in range(len(output)):\n",
    "    degree_assortativity_config_B.append(output[i][1])\n",
    "    attribute_assortativity_config_B.append(output[i][2])\n",
    "    \n",
    "\n",
    "print(f'lenght of degree_assortativity_config: {len(degree_assortativity_config_B)}')\n",
    "print(f'lenght of attribute_assortativity_config: {len(attribute_assortativity_config_B)}')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "fig.suptitle('Degree and Attribute Assortativity - Graph without Bot Node', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot Degree Assortativity\n",
    "axs[0].hist(degree_assortativity_config_B, bins=20, alpha=0.5, label='Random model')\n",
    "axs[0].axvline(degree_correlation, color='r', linestyle='dashed', linewidth=2, label='Original network')\n",
    "axs[0].set_xlabel('Assortativity coefficient')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Degree Assortativity - Graph without Bot Node', fontsize=12, weight='bold')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Attribute Assortativity\n",
    "axs[1].hist(attribute_assortativity_config_B, bins=20, alpha=0.5, label='Random model')\n",
    "axs[1].axvline(attribute_correlation, color='r', linestyle='dashed', linewidth=2, label='Original network')\n",
    "axs[1].set_xlabel('Assortativity coefficient')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Attribute Assortativity - Graph without Bot Node', fontsize=12, weight='bold')\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "fig.tight_layout()\n",
    "plt.savefig('degree_attribute_assortativity.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### largest component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest component of the network\n",
    "largest_cc = max(nx.connected_components(GB), key=len)\n",
    "largest_cc_random = max(nx.connected_components(Grb), key=len)\n",
    "\n",
    "# Create subgraphs\n",
    "largest_cc_subgraph = GB.subgraph(largest_cc)\n",
    "largest_cc_random_subgraph = Grb.subgraph(largest_cc_random)\n",
    "\n",
    "print(f'The largest connected component of the github collaboration network has {len(largest_cc)} nodes')\n",
    "print(f'The largest connected component of the Random Network has {len(largest_cc_random)} nodes')\n",
    "\n",
    "# see avarage shortest path\n",
    "shortest_path = nx.average_shortest_path_length(largest_cc_subgraph)\n",
    "shortest_path_random = nx.average_shortest_path_length(largest_cc_random_subgraph)\n",
    "\n",
    "print(f'The average shortest path length of the github collaboration network is {shortest_path}')\n",
    "print(f'The average shortest path length of the Random Network is {shortest_path_random}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_network = nw.get_filtered_network(GB,node_group_key='company')\n",
    "stylized_network, config = nw.visualize(stylized_network)\n",
    "# illustrative example of the network stylized_network\n",
    "nw.draw_netwulf(stylized_network)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
